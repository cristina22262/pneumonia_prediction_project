{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BigDL installation, Spark Session Creation and Loading Required Libraries"
      ],
      "metadata": {
        "id": "98OyHSiaoVYA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tytl_W4I3RtK",
        "outputId": "05aef67c-24a5-4eec-8245-5e29c80e8e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bigdl-dllib-spark3\n",
            "  Downloading bigdl_dllib_spark3-2.5.0b20240324-py3-none-manylinux1_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: numpy<=1.26.4,>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from bigdl-dllib-spark3) (1.26.4)\n",
            "Collecting pyspark==3.4.1 (from bigdl-dllib-spark3)\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting conda-pack==0.3.1 (from bigdl-dllib-spark3)\n",
            "  Downloading conda_pack-0.3.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from bigdl-dllib-spark3) (1.17.0)\n",
            "Collecting bigdl-core==2.4.0.dev0 (from bigdl-dllib-spark3)\n",
            "  Downloading bigdl_core-2.4.0.dev0-py3-none-manylinux2010_x86_64.whl.metadata (291 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from conda-pack==0.3.1->bigdl-dllib-spark3) (75.1.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==3.4.1->bigdl-dllib-spark3) (0.10.9.7)\n",
            "Downloading bigdl_dllib_spark3-2.5.0b20240324-py3-none-manylinux1_x86_64.whl (64.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bigdl_core-2.4.0.dev0-py3-none-manylinux2010_x86_64.whl (51.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conda_pack-0.3.1-py2.py3-none-any.whl (27 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285391 sha256=f6bf11981469052539c955fa3987efa370e9df89aebb7c25f157bd089a4a8b7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/b4/d8/38accc42606f6675165423e9f0236f8e825f6b6b6048d6743e\n",
            "Successfully built pyspark\n",
            "Installing collected packages: bigdl-core, pyspark, conda-pack, bigdl-dllib-spark3\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.4\n",
            "    Uninstalling pyspark-3.5.4:\n",
            "      Successfully uninstalled pyspark-3.5.4\n",
            "Successfully installed bigdl-core-2.4.0.dev0 bigdl-dllib-spark3-2.5.0b20240324 conda-pack-0.3.1 pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --pre --upgrade bigdl-dllib-spark3\n",
        "\n",
        "exit() # restart the runtime to refresh installed pkg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8mIbZPqk3UFo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from bigdl.dllib.nn.criterion import *\n",
        "from bigdl.dllib.nn.layer import *\n",
        "from bigdl.dllib.optim.optimizer import Adam\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType, StringType\n",
        "\n",
        "from bigdl.dllib.nncontext import *\n",
        "from bigdl.dllib.feature.image import *\n",
        "from bigdl.dllib.nnframes import *\n",
        "\n",
        "from optparse import OptionParser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_conf = SparkConf().set(\"spark.driver.memory\", \"20g\") \\\n",
        "            .set(\"spark.driver.cores\", 6)\n",
        "sc = init_nncontext(spark_conf, cluster_mode=\"local\")"
      ],
      "metadata": {
        "id": "jtv2ARBzpI--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Upload and Pre-processing"
      ],
      "metadata": {
        "id": "QDLrfMm9oey0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zupVh1CL3WNY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "9a5bde09-342b-41e3-cd51-c6b64b502335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-13655d99-2098-4788-9e48-f0ce3b2e5806\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-13655d99-2098-4788-9e48-f0ce3b2e5806\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "{\"username\":\"cristina12341234\",\"key\":\"7e103a7873d9d802decea3ad5418d3db\"}"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!cat ~/.kaggle/kaggle.json\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q9GMvGDN3YZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baea98d1-d557-423e-c754-9737096300c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming download from 55574528 bytes (2407790907 bytes left)...\n",
            "Resuming download from https://www.kaggle.com/api/v1/datasets/download/paultimothymooney/chest-xray-pneumonia?dataset_version_number=2 (55574528/2463365435) bytes left.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.29G/2.29G [00:24<00:00, 96.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train', 'val', 'test', 'chest_xray', '__MACOSX']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "imagepath = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
        "import os\n",
        "data_path = os.path.join(imagepath, 'chest_xray')\n",
        "print(os.listdir(data_path))\n",
        "\n",
        "\n",
        "train_path = os.path.join(data_path, 'train')\n",
        "val_path = os.path.join(data_path, 'val')\n",
        "test_path = os.path.join(data_path, 'test')\n",
        "\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType\n",
        "import os\n",
        "\n",
        "pneumonia_path = os.path.join(train_path, 'PNEUMONIA')\n",
        "normal_path = os.path.join(train_path, 'NORMAL')\n",
        "\n",
        "imageDF_train_p = NNImageReader.readImages(pneumonia_path, sc, resizeH=300, resizeW=300, image_codec=1) \\\n",
        "    .withColumn(\"label\", lit(2.0).cast(DoubleType()))  # Assign \"pneumonia\" as the label\n",
        "# Read normal images and add a \"label\" column\n",
        "imageDF_train_n = NNImageReader.readImages(normal_path, sc, resizeH=300, resizeW=300, image_codec=1) \\\n",
        "    .withColumn(\"label\", lit(1.0).cast(DoubleType()))  # Assign \"normal\" as the label\n",
        "# Combine both DataFrames\n",
        "imageDF_train = imageDF_train_p.union(imageDF_train_n)\n",
        "# for validation set\n",
        "imageDF_val_p = NNImageReader.readImages(os.path.join(val_path, 'PNEUMONIA'), sc, resizeH=300, resizeW=300, image_codec=1) \\\n",
        "    .withColumn(\"label\", lit(2.0).cast(DoubleType()))  # ✅ Match training labels\n",
        "imageDF_val_n = NNImageReader.readImages(os.path.join(val_path, 'NORMAL'), sc, resizeH=300, resizeW=300, image_codec=1) \\\n",
        "    .withColumn(\"label\", lit(1.0).cast(DoubleType()))  # ✅ Match training labels\n",
        "imageDF_val = imageDF_val_p.union(imageDF_val_n)\n",
        "# for test set\n",
        "imageDF_test_p = NNImageReader.readImages(os.path.join(test_path, 'PNEUMONIA'), sc, resizeH=300, resizeW=300, image_codec=1) \\\n",
        "    .withColumn(\"label\", lit(2.0).cast(DoubleType()))\n",
        "imageDF_test_n = NNImageReader.readImages(os.path.join(test_path, 'NORMAL'), sc, resizeH=300, resizeW=300, image_codec=1) \\\n",
        "    .withColumn(\"label\", lit(1.0).cast(DoubleType()))\n",
        "imageDF_test = imageDF_test_p.union(imageDF_test_n)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imageDF_train.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20as3uKtW5om",
        "outputId": "cd42335a-3453-4458-8208-d622375f07f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- image: struct (nullable = true)\n",
            " |    |-- origin: string (nullable = true)\n",
            " |    |-- height: integer (nullable = false)\n",
            " |    |-- width: integer (nullable = false)\n",
            " |    |-- nChannels: integer (nullable = false)\n",
            " |    |-- mode: integer (nullable = false)\n",
            " |    |-- data: binary (nullable = false)\n",
            " |-- label: double (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and Pipeline Definition"
      ],
      "metadata": {
        "id": "etSP1T2Co949"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TVNBTG_m3a8A"
      },
      "outputs": [],
      "source": [
        "batch_size = 56\n",
        "nb_epoch = 20\n",
        "learning_rate = 0.002\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7pvZ16Vc3g8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261d70fa-4c8c-427b-be25-ca7a964a0205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating: createRowToImageFeature\n",
            "creating: createImageResize\n",
            "creating: createImageCenterCrop\n",
            "creating: createImageChannelNormalize\n",
            "creating: createImageMatToTensor\n",
            "creating: createImageFeatureToTensor\n",
            "creating: createChainedPreprocessing\n",
            "creating: createTensorToSample\n",
            "creating: createChainedPreprocessing\n",
            "creating: createNNModel\n",
            "creating: createSequential\n",
            "creating: createLinear\n",
            "creating: createLogSoftMax\n",
            "creating: createClassNLLCriterion\n",
            "creating: createSeqToTensor\n",
            "creating: createScalarToTensor\n",
            "creating: createFeatureLabelPreprocessing\n",
            "creating: createNNClassifier\n",
            "creating: createAdam\n"
          ]
        }
      ],
      "source": [
        "transformer = ChainedPreprocessing(\n",
        "    [RowToImageFeature(), ImageResize(256, 256), ImageCenterCrop(224, 224),\n",
        "    ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(), ImageFeatureToTensor()])\n",
        "\n",
        "\n",
        "preTrainedNNModel = NNModel(Model.loadModel(\"/content/analytics-zoo_inception-v1_imagenet_0.1.0.model\"), transformer) \\\n",
        "    .setFeaturesCol(\"image\") \\\n",
        "    .setPredictionCol(\"embedding\")\n",
        "\n",
        "lrModel = Sequential().add(Linear(1000, 2)).add(LogSoftMax())\n",
        "\n",
        "classifier = NNClassifier(lrModel, ClassNLLCriterion(), SeqToTensor([1000])) \\\n",
        "    .setLearningRate(learning_rate) \\\n",
        "    .setOptimMethod(Adam()) \\\n",
        "    .setBatchSize(batch_size) \\\n",
        "    .setMaxEpoch(nb_epoch) \\\n",
        "    .setFeaturesCol(\"embedding\") \\\n",
        "    .setCachingSample(False) \\\n",
        "\n",
        "pipeline = Pipeline(stages=[preTrainedNNModel, classifier])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model training"
      ],
      "metadata": {
        "id": "1_ttHS8upnBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipeline.fit(imageDF_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvEGQL_7nzrE",
        "outputId": "99fa90dd-ce17-49b5-815d-2ce2f5fa2c20"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating: createToTuple\n",
            "creating: createChainedPreprocessing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validation and Prediction Error"
      ],
      "metadata": {
        "id": "CQg5ou4IpZp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictionDF = model.transform(imageDF_val).cache()\n",
        "predictionDF.sample(False, 0.1).show()"
      ],
      "metadata": {
        "id": "CUWxbth__UG7",
        "outputId": "e47b686c-881f-445a-fdd8-e4c1a968ebaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+--------------------+----------+\n",
            "|               image|label|           embedding|prediction|\n",
            "+--------------------+-----+--------------------+----------+\n",
            "|{file:/root/.cach...|  1.0|[7.494957E-5, 8.2...|       1.0|\n",
            "+--------------------+-----+--------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictionDF)\n",
        "    # expected error should be less than 10%\n",
        "print(\"Validation Error = %g \" % (1.0 - accuracy))\n",
        "\n"
      ],
      "metadata": {
        "id": "BmpfSgcXAAiC",
        "outputId": "97f0d544-f527-4966-d347-eb965af5ff4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Error = 0.3125 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testDF = model.transform(imageDF_test).cache()\n",
        "testDF.sample(False, 0.1).show()"
      ],
      "metadata": {
        "id": "Scc2M2lkAO9z",
        "outputId": "0ca37556-d169-4570-9826-0a9261bd0ed4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+--------------------+----------+\n",
            "|               image|label|           embedding|prediction|\n",
            "+--------------------+-----+--------------------+----------+\n",
            "|{file:/root/.cach...|  2.0|[1.2876233E-4, 2....|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[6.001197E-5, 1.1...|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[3.436058E-5, 7.2...|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[3.246968E-5, 3.9...|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[4.0650742E-5, 7....|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[2.8867991E-5, 1....|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[7.053932E-5, 1.8...|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[4.9989183E-5, 5....|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[3.5116405E-5, 1....|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[7.788882E-5, 9.6...|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[2.4530487E-4, 5....|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[2.2130687E-4, 1....|       1.0|\n",
            "|{file:/root/.cach...|  2.0|[5.505311E-5, 2.9...|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[1.3119873E-4, 1....|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[2.1646967E-5, 5....|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[3.8446437E-5, 9....|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[6.523761E-5, 1.1...|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[2.938562E-5, 2.8...|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[2.1912861E-5, 9....|       2.0|\n",
            "|{file:/root/.cach...|  2.0|[8.031376E-5, 9.5...|       2.0|\n",
            "+--------------------+-----+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(testDF)\n",
        "    # expected error should be less than 10%\n",
        "print(\"Validation Error = %g \" % (1.0 - accuracy))"
      ],
      "metadata": {
        "id": "uWcDdxIGAPXY",
        "outputId": "e0f59687-f338-4971-d7e7-834ee5d6090f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Error = 0.322115 \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}